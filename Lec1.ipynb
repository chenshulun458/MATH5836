{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Wrangling and Pre-processing\n",
    "\n",
    "- linear regression and logistic regression models with gradient descent learning algorithms\n",
    "\n",
    "-  evaluate the performance of the models using evaluation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent - find the local minimum of a function \n",
    "\n",
    "- second derivative -  plateau point + global minimum  but in more complex functions and models, that is not possible\n",
    "\n",
    "- 梯度是向量，梯度向量的每个分量表示在其他量不变时,f在xi方向上的变化率，对于单变量函数，就是在X轴的变换\n",
    "\n",
    "- 方向(梯度的反方向) 距离（学习率） 终止条件（最大迭代次数 或 |step| = |next_x - current_x| < precision（根据定义） ）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~source: https: //en.wikipedia.org/wiki/Gradient_descent\n",
    "# code source: https://en.wikipedia.org/w/index.php?title=Gradient_descent&oldid=966271567\n",
    "\n",
    "next_x = 6# We start the search at x = 6\n",
    "gamma = 0.01# Step size multiplier\n",
    "precision = 0.00001# Desired precision of result\n",
    "max_iters = 10000# Maximum number of iterations\n",
    "\n",
    "# 手动输入Derivative function\n",
    "def df(x):\n",
    "  return 4 * x ** 3 - 9 * x ** 2\n",
    "\n",
    "# 迭代计算 小于精度或超出最大迭代数时停止\n",
    "for i in range(max_iters):\n",
    "    current_x = next_x\n",
    "    next_x = current_x - gamma * df(current_x)\n",
    "    print(i, next_x, df(current_x))\n",
    "\n",
    "    step = next_x - current_x\n",
    "    if abs(step) <= precision:\n",
    "        break\n",
    "\n",
    "print(\"Minimum at \", next_x)\n",
    "\n",
    "# The output for the above will be something like \n",
    "# \"Minimum at 2.2499646074278457\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "SSE:\n",
    "$$\n",
    "\\mathbb{E}[ w ] \\equiv \\frac{1}{2} \\sum_{d \\in D} (t_d - o_d)^2\n",
    "$$\n",
    "\n",
    "\n",
    "MSE:\n",
    "$$\n",
    "\\text{Error}_{(m,b)} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - (mx_i + b))^2\n",
    "$$\n",
    "\n",
    "\n",
    "Note that both SSE and MSE can be used for linear regression. SSE loss is mostly used in the neural network literature, while MSE is used more in the statistics literature. The SSE has 1/2 normalising constant in front so that the derivative is simpler. \n",
    "\n",
    "adjust the parameters：\n",
    "- 1 find gradient\n",
    "$$\n",
    "\\frac{\\partial}{\\partial m} = \\frac{2}{N} \\sum_{i=1}^{N} -x_i (y_i - (mx_i + b))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial b} = \\frac{2}{N} \\sum_{i=1}^{N} -(y_i - (mx_i + b))\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "# 计算Error : MSE\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "# 计算gradient 和 新的b，m - gradient descent\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m]\n",
    "\n",
    "# 更新参数 b, m\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "        error = compute_error_for_line_given_points(b,m, points)\n",
    "        print(i, b, m, error,  'i, b, m, error')\n",
    "    return [b, m]\n",
    "\n",
    "def run():\n",
    "    points = genfromtxt(r\"data\\data_linearreg.csv\", delimiter=\",\")\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_m = 0 # initial slope guess\n",
    "    num_iterations = 1000\n",
    "    print (\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    print (\"Running...\")\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    print (\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)) )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X is a vector\n",
    "$$\n",
    "o = w_0 + w_1 x_1 + \\dots + w_n x_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[w] \\equiv \\frac{1}{2} \\sum_{d \\in D} (t_d - o_d)^2\n",
    "$$\n",
    "\n",
    "Our goal is to adjust the parameters of the linear model (w_0, w_1, w_2, ... , w_N) for the given input data (x_1, x_2, ... , x_N).\n",
    "\n",
    "- 求Gradient\n",
    "$$\n",
    "\\nabla E[w] \\equiv \\left[ \\frac{\\partial E}{\\partial w_0}, \\frac{\\partial E}{\\partial w_1}, \\dots, \\frac{\\partial E}{\\partial w_n} \\right]\n",
    "$$\n",
    "\n",
    "- Training rule:\n",
    "$$\n",
    "\\Delta w = -\\eta \\nabla E[w]\n",
    "$$\n",
    "\n",
    "i.e.,\n",
    "\n",
    "$$\n",
    "\\Delta w_i = -\\eta \\frac{\\partial E}{\\partial w_i}\n",
    "$$\n",
    "\n",
    "\n",
    "see: https://folk.idi.ntnu.no/keithd/classes/advai/lectures/backprop.pdf\n",
    "\n",
    "再看 推导\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson correlation coefficient(PCC) Linear relationship\n",
    "- strength of the relationship between two variables -1 ： 1\n",
    "- 0 : no linear relationship \n",
    "$$\n",
    "r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum (y_i - \\bar{y})^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-Squared Score\n",
    "- R-squared is the percentage of the response variable variation that is explained by a linear model.\n",
    "- 0% - 100%\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum (y_i - \\hat{y_i})^2}{\\sum (y_i - \\bar{y})^2}=1 - \\frac{SSR}{SST}\n",
    "$$\n",
    "\n",
    "- SSR 是残差平方和，表示模型未解释的变异量。\n",
    "- SST 是总变异，表示响应变量的所有变异。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
